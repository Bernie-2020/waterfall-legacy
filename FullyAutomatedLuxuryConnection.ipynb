{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:18:54.701800Z",
     "start_time": "2020-02-18T18:18:52.301390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import mobilize_import as mi\n",
    "import importlib\n",
    "import pyodbc\n",
    "import pyperclip\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from time import sleep\n",
    "from IPython.display import display, HTML\n",
    "from getpass import getpass\n",
    "\n",
    "importlib.reload(mi);\n",
    "def dh(x): display(HTML(x))\n",
    "def refresh(crsr): crsr.execute('select 0').fetchall()\n",
    "dh(\"<style>.container { width:75% !important; }</style>\")\n",
    "\n",
    "def split_into_chunks(filename, limit=int(18*1024*1024)):\n",
    "    filesize = os.path.getsize(filename)\n",
    "    chunks = math.ceil(filesize / limit)\n",
    "    chunksize= int(filesize / chunks)+1\n",
    "    print(f\"Splitting {filename} into {chunks} chunks.\")\n",
    "    with open(filename, 'rb') as fin:\n",
    "        firstline = fin.readline()\n",
    "        start = fin.tell()\n",
    "        for i in range(chunks):\n",
    "            sp = os.path.splitext(filename)\n",
    "            filename_new = sp[0]+f'_{i+1}'+sp[1]\n",
    "            with open(filename_new, 'wb') as fout:\n",
    "                fout.write(firstline)\n",
    "                offset = fin.seek( min(filesize,start+chunksize) ) - start\n",
    "                if fin.tell() < filesize:\n",
    "                    fin.readline()\n",
    "                    offset = fin.tell() - start\n",
    "                fin.seek(start)\n",
    "                fout.write(fin.read(offset))\n",
    "                print(f\"Writing file {os.path.basename(filename_new)} of {offset/(2**20):.2f}MB.\")\n",
    "                start += offset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Database connection\n",
    "Connect to the database by running the following three cells, inputting your password when prompted.  If the final cell doesn't succeed, you can't connect to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:18:57.612720Z",
     "start_time": "2020-02-18T18:18:54.702802Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db password:········\n"
     ]
    }
   ],
   "source": [
    "password=getpass(\"db password:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:18:58.534756Z",
     "start_time": "2020-02-18T18:18:58.532754Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cnxn=None\n",
    "cstring=\"Driver={Amazon Redshift (x64)}; \" \\\n",
    "        \"Server=redshift.zigzagcanal.orgs.civis.io; \" \\\n",
    "        \"Database=dev;\" \\\n",
    "        \"UID=tmcmanus;\" \\\n",
    "        f\"PWD={password}; \" \\\n",
    "        \"Port=5432\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:34:20.464042Z",
     "start_time": "2020-02-18T18:34:20.033209Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if cnxn: cnxn.close()\n",
    "cnxn = pyodbc.connect(cstring, autocommit=True)\n",
    "crsr = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Choosing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Replace `job` and `job_name` with the job ID and a job descriptor (prepended with `fld_` or `natl_`)  Set `csv_path` to the fully qualified directory where you intend to save the final CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:34:12.705700Z",
     "start_time": "2020-02-18T18:34:12.702706Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waterfall.t20200218_716_fld_nc_oo\n"
     ]
    }
   ],
   "source": [
    "job=716\n",
    "job_name='fld_nc_oo'\n",
    "csv_path=r'C:\\Users\\mcman\\OneDrive\\Desktop\\Work\\Drive\\Texting Lists\\waterfall'\n",
    "table_name=f'waterfall.t{dt.date.today():%Y%m%d}_{job}_{job_name}'\n",
    "print(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(Optional) Run the following cell to view all the requests in the job you chose.  Review `ak_title` and `combo` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:34:06.511760Z",
     "start_time": "2020-02-18T18:34:06.477735Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nselect job_id, request_id, mobilize_id, distance, capacity, earliest, latest, ak_title as title, combo, 'b-2020.com'||event1_link as link \nfrom waterfall.job_requests\nwhere job_id in (716)': ('00000', '[00000] [Amazon][Amazon Redshift] (30) Error occurred while trying to execute a query: [SQLState 00000] another command is already in progress\\n (30) (SQLExecDirectW)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1594\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1595\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1596\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: ('00000', '[00000] [Amazon][Amazon Redshift] (30) Error occurred while trying to execute a query: [SQLState 00000] another command is already in progress\\n (30) (SQLExecDirectW)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a9c8e3a24fef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwaterfall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_requests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m where job_id in ({job})\"\"\"\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcnxn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m             \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         )\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1608\u001b[0m                 \u001b[1;34m\"Execution failed on sql '{sql}': {exc}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m             )\n\u001b[1;32m-> 1610\u001b[1;33m             \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[1;34m(exc, traceback)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1593\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1594\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1595\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1596\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nselect job_id, request_id, mobilize_id, distance, capacity, earliest, latest, ak_title as title, combo, 'b-2020.com'||event1_link as link \nfrom waterfall.job_requests\nwhere job_id in (716)': ('00000', '[00000] [Amazon][Amazon Redshift] (30) Error occurred while trying to execute a query: [SQLState 00000] another command is already in progress\\n (30) (SQLExecDirectW)')"
     ]
    }
   ],
   "source": [
    "sql = f\"\"\"\n",
    "select job_id, request_id, mobilize_id, distance, capacity, earliest, latest, ak_title as title, combo, 'b-2020.com'||event1_link as link \n",
    "from waterfall.job_requests\n",
    "where job_id in ({job})\"\"\"\n",
    "pd.read_sql(sql,cnxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Automation\n",
    "Run the following cells to execute the importer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This cell creates the importer.  Add any exclusions to `exclusions`.  Change `implicit_exclusions` to `false` if you don't want to automatically include exclusions from lists for events that haven't happened yet in the same state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:33:21.708832Z",
     "start_time": "2020-02-18T18:33:21.673801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "importer = mi.create_importer(table_name=table_name,\n",
    "                            job_id_in=job,\n",
    "                            exclusions=[],\n",
    "#                             title_override='Organize your Friends Party',\n",
    "                            importer_type='watf',\n",
    "#                             importer_type='bulk',\n",
    "\n",
    "                            date_abs=[20200219,20200225]\n",
    "                           )\n",
    "\n",
    "# Uncomment and use this for non-waterfall pulls\n",
    "# importer = mi.create_importer(table_name=table_name,\n",
    "#                                 mob_id_in=ids,\n",
    "#                                 job_id = job, #This is for job_tables\n",
    "#                                 date_abs = ['20200107','20200111'],\n",
    "#                                 exclusions=['waterfall.t20200104_EXCLUSION_SD_OOS',\n",
    "#                                             'waterfall.t20200104_EXCLUSION_kc_OOS',\n",
    "#                                             'waterfall.t20200104_EXCLUSION_linc_OOS'],\n",
    "#                                 distance=40,\n",
    "#                                 capacity=60,\n",
    "#                                 importer_type='base'\n",
    "#                            )\n",
    "\n",
    "# Run this to copy the query to your clipboard\n",
    "# importer.full_query(True); \n",
    "\n",
    "implicit_exclusions= True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This cell runs through the verify step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:33:42.209662Z",
     "start_time": "2020-02-18T18:33:27.545613Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8ed40084d92b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcrsr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnxn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcrsr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcrsr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrsr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# df = pd.read_sql('select job_id, request_id, event_id, mobilize_id, event_category, event_title, event_state, event_date, capacity, event_distance_override, combo from events', cnxn)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "crsr = cnxn.cursor() \n",
    "crsr.execute(importer.input)\n",
    "crsr.execute(importer.events)\n",
    "refresh(crsr)\n",
    "# df = pd.read_sql('select job_id, request_id, event_id, mobilize_id, event_category, event_title, event_state, event_date, capacity, event_distance_override, combo from events', cnxn)\n",
    "df = pd.read_sql('select * from events', cnxn)\n",
    "if len(df) == 0:\n",
    "    raise Exception(\"No events found\")\n",
    "else:\n",
    "    print(\"EVENTS:\")\n",
    "    dh(df.to_html(index=False))\n",
    "    dh('<hr>')\n",
    "    \n",
    "\n",
    "if(implicit_exclusions):\n",
    "    states = '\\',\\''.join(list(df.event_state.drop_duplicates())+['NATL'])\n",
    "    sql = f\"select distinct table_name from waterfall.job_tables where state in ('{states}') and event_date > getdate() and table_name <> '{table_name}'\" \n",
    "    df_excl = pd.read_sql(sql, cnxn)\n",
    "    excl = df_excl['table_name']\n",
    "    importer.exclusions += list(excl)\n",
    "    importer.make_exclusions()\n",
    "    if not excl.empty:\n",
    "        print(\"EXCLUSIONS:\")\n",
    "        print('\\n'.join(importer.exclusions))\n",
    "        dh('<hr>')\n",
    "    \n",
    "\n",
    "    \n",
    "val = crsr.execute(importer.event_proximity).execute('select count(*) from event_proximity').fetchval();\n",
    "crsr.fetchall()\n",
    "if val:\n",
    "    print(\"Event_proximity count: \",val)\n",
    "    dh('<hr>')\n",
    "else:\n",
    "    raise Exception(\"event_proximity empty\")\n",
    "    \n",
    "crsr.execute(importer.user_assignment)\n",
    "refresh(crsr)\n",
    "df2 = pd.read_sql(importer.verify, cnxn)\n",
    "print(\"VERIFY:\")\n",
    "dh(df2.to_html(index=False))\n",
    "dh('<hr>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This cell creates the final table, with the table name you created at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T01:44:57.499204Z",
     "start_time": "2020-02-06T01:44:57.491416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# crsr.execute(f\"update {table_name} set event_title1='Debate Watch Party' where event_campaign1 ilike '%debate%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:54:54.654953Z",
     "start_time": "2020-02-06T03:54:32.753467Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table size: 165662\n"
     ]
    }
   ],
   "source": [
    "fcount = crsr.execute(importer.final_table).execute(importer.job_tables).execute(f\"select count(*) from {table_name}\").fetchval()\n",
    "crsr.fetchall()\n",
    "print(\"Final table size:\",fcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This cell downloads the table and saves it to a CSV.  Not tested for large tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:55:12.498114Z",
     "start_time": "2020-02-06T03:54:54.659954Z"
    }
   },
   "outputs": [],
   "source": [
    "schema, no_schema = table_name.split('.') \n",
    "final_df = pd.read_sql(f\"select * from {table_name}\", cnxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T19:27:56.823229Z",
     "start_time": "2020-01-15T19:27:56.821235Z"
    }
   },
   "source": [
    "This cell exports to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:55:12.518109Z",
     "start_time": "2020-02-06T03:55:12.499137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waterfall.t20200205_492_fld_ca_canvasses\n"
     ]
    }
   ],
   "source": [
    "pyperclip.copy(table_name)\n",
    "print(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:55:12.562625Z",
     "start_time": "2020-02-06T03:55:12.520619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           Sunday, Feb 9\n",
      "56        Saturday, Feb 8\n",
      "51572    Thursday, Feb 13\n",
      "Name: day1, dtype: object\n",
      "0    Canvass\n",
      "Name: event_title1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(final_df['day1'].drop_duplicates())\n",
    "print(final_df['event_title1'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:50:34.349703Z",
     "start_time": "2020-02-06T03:50:34.333715Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df['event_title1'] = final_df['event_title1'].apply(lambda x: 'Canvass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:55:59.283751Z",
     "start_time": "2020-02-06T03:55:57.564665Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.to_csv(os.path.join(csv_path, no_schema+'.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells let you split out the result by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T20:48:48.716924Z",
     "start_time": "2020-01-05T20:48:47.187011Z"
    }
   },
   "outputs": [],
   "source": [
    "cities = list(final_df['event1_city'].drop_duplicates())\n",
    "for city in cities:\n",
    "    final_df.loc[final_df['event1_city'] == city].to_csv(os.path.join(csv_path, no_schema+f'_{city}.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells show you how to split out the result csv by `event1_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:21:38.107478Z",
     "start_time": "2020-02-06T03:21:38.095374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Thursday, Feb 6\n",
      "1 Friday, Feb 7\n",
      "2 Saturday, Feb 8\n",
      "3 Sunday, Feb 9\n",
      "4 Monday, Feb 10\n",
      "5 Tuesday, Feb 11\n",
      "6 Wednesday, Feb 12\n",
      "7 Thursday, Feb 13\n",
      "8 Friday, Feb 14\n"
     ]
    }
   ],
   "source": [
    "dates = final_df['day1'].drop_duplicates()\n",
    "dl = sorted(list(dates), key=lambda x: x[-2:])\n",
    "for i, d in enumerate(dl):\n",
    "    print(i, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:21:55.135713Z",
     "start_time": "2020-02-06T03:21:55.130695Z"
    }
   },
   "outputs": [],
   "source": [
    "strides = [0,4, len(dl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T03:21:55.885403Z",
     "start_time": "2020-02-06T03:21:55.810007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting events from Thursday, Feb 6 through Sunday, Feb 9 (inclusive).\n",
      "C:\\Users\\mcman\\OneDrive\\Desktop\\Work\\Drive\\Texting Lists\\waterfall\\t20200205_521_522_fld_volr_phonebanks_1.csv... Success!\n",
      "Exporting events from Monday, Feb 10 through Friday, Feb 14 (inclusive).\n",
      "C:\\Users\\mcman\\OneDrive\\Desktop\\Work\\Drive\\Texting Lists\\waterfall\\t20200205_521_522_fld_volr_phonebanks_2.csv... Success!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(strides)):\n",
    "    s = (strides[i-1], strides[i])\n",
    "    print(f\"Exporting events from {dl[s[0]]} through {dl[s[1]-1]} (inclusive).\")\n",
    "    fname = os.path.join(csv_path, no_schema+f'_{i}.csv')\n",
    "    print(fname, end='... ')\n",
    "    final_df.loc[final_df['day1'].isin(dl[s[0]:s[1]])].to_csv(fname)\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
